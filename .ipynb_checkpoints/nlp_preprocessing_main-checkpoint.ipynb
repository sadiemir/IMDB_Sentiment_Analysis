{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f214375-c5a2-4c6a-8abd-fb00ec2f0b3a",
   "metadata": {},
   "source": [
    "#### Step 1: Download the IMDB dataset and take a look at it. To keep the notebool lightweight, we only grab a part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9be36bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#load the IMDP dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "#take a look at the data structure\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714237e-0a5e-4acd-8ee9-af582be42369",
   "metadata": {},
   "source": [
    "To ensure the noteook remains small, we can work with a subset of the data for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb3bca0b-e77a-4a7d-9f78-524c41eb87cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "#take a small subset for training and testing\n",
    "train_data = imdb_dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "test_data = imdb_dataset['test'].shuffle(seed=42).select(range(500))\n",
    "\n",
    "#preview the first example\n",
    "print(train_data[0])                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa824312-6b54-4d5b-b486-0f0e431609ea",
   "metadata": {},
   "source": [
    "#### Step 2: Preprocessing the traina and test data. This step includes tokenization, stemming, lemmatization, and removing stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88432540-025e-44ef-9f93-00e1f068ffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['relat fortier profil fact polic seri violent profil look crispi fortier look profil plot quit fortier plot far complic fortier look like prime suspect spot similar main charact weak weirdo clairvoy peopl like compar judg enjoy funni thing peopl write fortier look american hand argu prefer american seri mayb languag spirit think thi seri english way actor realli good act superfici', 'thi movi plot veri true book classic write mark movi start scene hank sing song bunch kid call stub toe moon remind sinatra song high hope fun music great throughout favorit song sung king hank bing crosbi sir saggi overal great famili movi even great date thi movi watch princess play rhonda fleme love thi movi like danni kay court jester definit like thi movi']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/smirghor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/smirghor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/smirghor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries for NLP processing\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer #for breaking text into words\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer #for stemming and lemmatization\n",
    "from nltk.corpus import wordnet, stopwords #for accessing word meaning and stopwords\n",
    "from nltk import pos_tag #for part of speech tagging\n",
    "\n",
    "#download required resources for tokenization, lemmatization, and stop words removal\n",
    "nltk.data.path.append('/Users/smirghor/nltk_data')\n",
    "nltk.download('wordnet') \n",
    "nltk.download('averaged_perceptron_tagger_eng') #POS tagger for lemmatization\n",
    "nltk.download('stopwords') #stop words list\n",
    "\n",
    "#innitialize the stemmer and lemmatizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) #define a set of common english stop words\n",
    "\n",
    "\n",
    "# Function to convert POS tags to a format suitable for WordNet lemmatization\n",
    "def get_wordnet_pos(tag):\n",
    "    # Map POS tag to WordNet format for accurate lemmatization\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if unknown\n",
    "\n",
    "\n",
    "# Function to preprocess a given text \n",
    "def preprocess_text(text):\n",
    "    # 1. tokenize the text into words\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # 2. apply stemming\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token.isalpha()]\n",
    "    #3. apply lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos_tag([token])[0][1])) for token in tokens]\n",
    "    #4. remove stop words\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    # join the token back into a single string and return it\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "#Example: apply preprocessing to a few samples from training data\n",
    "preprocessed_train_data = [preprocess_text(review) for review in train_data['text'][:10]]\n",
    "\n",
    "#display the first two preprocessed reviews to see the results\n",
    "print(preprocessed_train_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04f1fff3-9955-4bbb-b559-9d16d57aeef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the training data (subset of 1000)\n",
    "preprocessed_train_data = [preprocess_text(review['text']) for review in train_data]\n",
    "\n",
    "# Apply preprocessing to the test data (subset of 500)\n",
    "preprocessed_test_data = [preprocess_text(review['text']) for review in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b39620-83c8-4f35-89df-e218550c6b8c",
   "metadata": {},
   "source": [
    "#### Step 3: Transforming Text Data into TF-IDF Features (to convert the preprocessed text data into numerical features suitable for modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7eee348-22f9-49e6-8893-fbffb662664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape(train): (1000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  #limit to 5000 features to keep it manageable\n",
    "\n",
    "# fit and transform the preprocessed training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(preprocessed_train_data) \n",
    "\n",
    "# transform the test data \n",
    "X_test_tfidf = tfidf_vectorizer.transform(preprocessed_test_data)\n",
    "\n",
    "#extract the labels (0=negative, 1=positive)\n",
    "y_train = [review for review in train_data['label'][:1000]]\n",
    "y_test = [review for review in test_data['label'][:500]]\n",
    "\n",
    "#display the shape of the TF-IDF feature matrix\n",
    "print(f\"TF-IDF matrix shape(train): {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b934404-532d-4a9a-9bec-057cba2365c8",
   "metadata": {},
   "source": [
    "#### Step 4: Building a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0661e-71e2-4cac-9775-31e67e4c6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#initialize the logistic regression model \n",
    "logistic_model = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
